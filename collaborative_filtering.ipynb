{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports the libraries\n",
    "import csv\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import math\n",
    "import numpy as np\n",
    "import heapq\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/jakob/OneDrive - Universitat Ramón Llull/ESADE/Term3/RecommenderSystems/Session2_CollaborativeFiltering/Lab_1.2_CollaborativeFiltering'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# builds the critics data set\n",
    "input_file = csv.DictReader(open(f\"{data_path}/DataSets/movieratings.csv\"))\n",
    "critics = {}\n",
    "for row in input_file:\n",
    "    key = row.pop('User')\n",
    "    if not key in critics:\n",
    "        critics[key] = {}\n",
    "        for item in row:\n",
    "            if row[item]!='':critics[key][item] = float(row[item])\n",
    "                \n",
    "#transform critics dict into pd.df once in the beginning instead of in every function call in order to save computing cost\n",
    "critics = pd.DataFrame.from_dict(critics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>user id</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>934</th>\n",
       "      <th>935</th>\n",
       "      <th>936</th>\n",
       "      <th>937</th>\n",
       "      <th>938</th>\n",
       "      <th>939</th>\n",
       "      <th>940</th>\n",
       "      <th>941</th>\n",
       "      <th>942</th>\n",
       "      <th>943</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Movie</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000: Lightning Jack (1994)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001: Stupids, The (1996)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002: Pest, The (1997)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003: That Darn Cat! (1997)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004: Geronimo: An American Legend (1993)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997: Stuart Saves His Family (1995)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998: Cabin Boy (1994)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999: Clean Slate (1994)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99: Snow White and the Seven Dwarfs (1937)</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9: Dead Man Walking (1995)</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1682 rows × 943 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "user id                                     1    2    3    4    5    6    7    \\\n",
       "Movie                                                                           \n",
       "1000: Lightning Jack (1994)                 NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "1001: Stupids, The (1996)                   NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "1002: Pest, The (1997)                      NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "1003: That Darn Cat! (1997)                 NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "1004: Geronimo: An American Legend (1993)   NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "...                                         ...  ...  ...  ...  ...  ...  ...   \n",
       "997: Stuart Saves His Family (1995)         NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "998: Cabin Boy (1994)                       NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "999: Clean Slate (1994)                     NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "99: Snow White and the Seven Dwarfs (1937)  3.0  NaN  NaN  NaN  3.0  NaN  5.0   \n",
       "9: Dead Man Walking (1995)                  5.0  NaN  NaN  NaN  NaN  4.0  5.0   \n",
       "\n",
       "user id                                     8    9    10   ...  934  935  936  \\\n",
       "Movie                                                      ...                  \n",
       "1000: Lightning Jack (1994)                 NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "1001: Stupids, The (1996)                   NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "1002: Pest, The (1997)                      NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "1003: That Darn Cat! (1997)                 NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "1004: Geronimo: An American Legend (1993)   NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "...                                         ...  ...  ...  ...  ...  ...  ...   \n",
       "997: Stuart Saves His Family (1995)         NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "998: Cabin Boy (1994)                       NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "999: Clean Slate (1994)                     NaN  NaN  NaN  ...  NaN  NaN  NaN   \n",
       "99: Snow White and the Seven Dwarfs (1937)  NaN  NaN  5.0  ...  3.0  NaN  NaN   \n",
       "9: Dead Man Walking (1995)                  NaN  NaN  4.0  ...  NaN  1.0  4.0   \n",
       "\n",
       "user id                                     937  938  939  940  941  942  943  \n",
       "Movie                                                                          \n",
       "1000: Lightning Jack (1994)                 NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1001: Stupids, The (1996)                   NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1002: Pest, The (1997)                      NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1003: That Darn Cat! (1997)                 NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1004: Geronimo: An American Legend (1993)   NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "...                                         ...  ...  ...  ...  ...  ...  ...  \n",
       "997: Stuart Saves His Family (1995)         NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "998: Cabin Boy (1994)                       NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "999: Clean Slate (1994)                     NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "99: Snow White and the Seven Dwarfs (1937)  NaN  NaN  NaN  NaN  NaN  5.0  NaN  \n",
       "9: Dead Man Walking (1995)                  5.0  3.0  5.0  3.0  NaN  NaN  3.0  \n",
       "\n",
       "[1682 rows x 943 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#100k movie lense dataset\n",
    "\n",
    "headers = ['user id', 'item id', 'rating', 'timestamp'] \n",
    "rating_raw = pd.read_table(f'{data_path}/DataSets/extra/ml-100k/u.data', header=None, names=headers)\n",
    "rating_raw.drop(['timestamp'], axis=1, inplace=True)\n",
    "\n",
    "headers = ['movie id', 'movie title'] \n",
    "title = pd.read_table(f'{data_path}/DataSets/extra/ml-100k/u.item', encoding='latin-1', header=None, sep='|')\n",
    "title = title.iloc[:, :2]\n",
    "title.columns = ['MovieId', 'MovieTitle']\n",
    "title[\"Movie\"] = title['MovieId'].astype(str) + ': ' + title['MovieTitle']\n",
    "\n",
    "critics_100k = rating_raw.merge(title, how='left', left_on='item id', right_on='MovieId')\n",
    "\n",
    "critics_100k.drop(['item id', 'MovieId', 'MovieTitle'], axis=1, inplace=True)\n",
    "critics_100k['user id'].astype(str)\n",
    "\n",
    "critics_100k = critics_100k.pivot(index='Movie', columns='user id', values='rating')\n",
    "\n",
    "critics_100k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation of the Pearson Correlation\n",
    "\n",
    "The following Cell computes the pearson Correlation of a given pair of Persons. The Correlation is hereby computed as a correlation dataframe using the .corr method included in Pandas. The pandas specific method is chosen over e.g. scipy.stats.pearsonr since it can handle the dataframe more efficiently for example in terms of NA values (The Pandas Method is overall a natural fit with Pandas Dataframes).\n",
    "\n",
    "Furthermore the correlation matrix provides two options that should be considered against each other in terms of computational efficiency:\n",
    "1. Computing **one** complete correlation matrix of all users **once** and calling the desired pair from the matrix when needed.\n",
    "2. Extracting the desired users from the total dataframe, computing the correlation matrix for solely these but in **every** iteration and calling the correlation from the 2x2 matrix respectively.\n",
    "    \n",
    "Testing both approaches for efficiency usign the %%timeit iPython module resulted in the following (testing times for the sample dataset):\n",
    "\n",
    "##### Approach 1:\n",
    "392 µs ± 2.44 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
    "\n",
    "##### Approach 2:\n",
    "902 µs ± 16.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) \n",
    "\n",
    "Following the clearly higher efficiency of approach 1, I continued using this method for the computation. This resulted in separating the original function by defining the complete correlation matrix outside the pearson function, so it is computed once in the beginning. The pearson function, which is called later, then solely extracts the respective pairs from the existing complete matrix.\n",
    "\n",
    "In the final algorithms the pearsonSimilarity Martix is not neeeded anymore as all computations are done as matrix computations instead of single callouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "\n",
    "def pearsonMatrix(prefs):\n",
    "    \"\"\"\n",
    "    Computes the complete pearson correlation Matrix.\n",
    "    \"\"\"\n",
    "    matrix = prefs.corr(method='pearson') #.corr computes pearson correlation of all inputted pairs and outputs in matrix\n",
    "    return matrix #returns complete correlation matrix\n",
    "\n",
    "def pearsonSimilarity(person1, person2, corrM):\n",
    "    \"\"\"\n",
    "    Calls the desired pair of persons from the complete correlation Matrix.\n",
    "    \"\"\"\n",
    "    similarity = corrM.loc[person1, person2] #.loc calls the desired pair of persons from the complete correlation Matrix\n",
    "    return similarity #returns single correlation value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate Ratings and Extract Recommendations per User\n",
    "In the following Cell the missing ratings are estimated for a given User. \n",
    "\n",
    "As for loops are usually a highly inefficient way of working with table data I aimed to make as many computations as possible in dataframe/ vector format.\n",
    "\n",
    "In order to compute the V weights the complete correlation Matrix is passed to getRecommendations. I created a dicttionary from the row of correlationts that represent the correlatin of all Vs with given U. This dictionary is then apended to the rating matrix as an additional row (index ='similarity_v').\n",
    "\n",
    "I was following the formular of personalized & normalized collaboratove filtering (S = sum((r - mean(rV))* w) / sum(w)). Therefore, I firstly needed to compute the product of all of V's ratings subtracted by V's rating mean and the similarity between U and the respective V for every item that is to be estimated. The rating is normalized by adding the mean rating as a new row to the dataframe and subtracting it in the iteration over all the estimation targets. Since we included W as 'similarity_v' in the main dataframe this figure needs to be multiplied with each row (representing the ratings per movie) of movie ratings. To do this I iterate over the movies that need to be estimated (not rated by U so far) and use these as index for the respectve vector multiplication. The sum of each iteration is then divided by the sum of the entire 'similarity_v' (division done as part of iteration). For each iteration the resulting score is inserted in the main dataframe for U & the respective movie in order to save memory power (in comparison to save it in e.g. an extra list).\n",
    "\n",
    "The entire column U of the main matrix is then inputtet into a scaler (sklearn MinMaxScaler) in order to adapt tyhe estimations for the given 1-5 scale.\n",
    "\n",
    "The not rated movienames as well as the estimations are then written into the desired outpur format (list of tuples).\n",
    "In the final step this list is sorted (descending) and filtered for the top N results which represent the recommendations to U.\n",
    "\n",
    "##### In the experimentation process following efficiency steps have been taken:\n",
    "Initial approach with nested loop over users and movies:\n",
    "30.5 ms ± 1.89 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
    "\n",
    "Replacing pandas dfs with arrays for computations whereever possible:\n",
    "24.2 ms ± 173 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
    "\n",
    "Replacing nested loops with vector multiplication:\n",
    "8.67 ms ± 523 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
    "\n",
    "Including scaling & output format:\n",
    "9.33 ms ± 45.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRecommendations(prefs,person, similarity_matrix):  \n",
    "    \"\"\"\n",
    "    Computing an estimation for the missing rating for a given User U.\n",
    "    \"\"\"\n",
    "    ## Appending Similarities (W) as weight row to main dataframe\n",
    "    similarity_dict = {} #define dict with similarities to person\n",
    "    for i in prefs.columns: similarity_dict[i] = similarity_matrix.loc[person, i] #append similarity of every user in main dataframe to dict\n",
    "    prefs = prefs.append(pd.Series(similarity_dict,name='similarity_v')) #appending similarities of V & U as new row to main dataframe   \n",
    "     \n",
    "    ## Compute mean movie rating given by V in order to normalise later\n",
    "    prefs.loc['mean_rating_v'] = prefs.mean(axis=0) #Compute V rating mean in new row\n",
    "    \n",
    "    ## Computing the sum of weights (Denominator in formula) \n",
    "    sum_of_v_weights = prefs.loc['similarity_v'].sum() #summing up the row entries in weight row\n",
    "        \n",
    "    ## Define list of movies that have not been rated by U and therefore need to be iterated\n",
    "    not_rated = prefs[person][prefs[person].isna()].index #extract list of movies not rated by U (need to be estimated)\n",
    "        \n",
    "    ## Iterate movie names that have not been rated by U\n",
    "    for m in not_rated: #for every movie that was not rated\n",
    "        prefs.loc['normalised_rating'] = prefs.loc[m, :].values - prefs.loc['mean_rating_v'].values #compute normalised rating\n",
    "        prefs.loc['weighted_rating'] = prefs.loc['normalised_rating'].values * prefs.loc['similarity_v', :].values #movie rating of V * Wvu (create new row / overwrite row with all results for movie m of iteration)           \n",
    "        prefs.loc[m, person] = np.nansum(prefs.loc['weighted_rating'].values) / sum_of_v_weights #U score estimation by summing all weithed ratings, dividing by sum of weights & inserting in respective plave in U column\n",
    "            \n",
    "    ## Scale Estimated Ratings on the given scale of 0-5\n",
    "    scaler = MinMaxScaler((0, 5)) #define scaler (sklearn package)\n",
    "    prefs[person] = scaler.fit_transform(prefs[[person]]) #insert U column (containing estimations into scaler)\n",
    "    \n",
    "    ## Write Results in output format (List of Tuples)\n",
    "    rankings = [] #define list to append tuples \n",
    "    for m in not_rated: rankings.append((prefs.loc[m, person], m)) #append estimated pair of movie name and estimated rating as tuple to list\n",
    "                \n",
    "    return sorted(rankings, reverse = True)[:10] #return sorted list of tuples (reverse = True for descending order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neighbourhood Selection\n",
    "The following cell computes the neighbourhood to user U that can then be considered to compute recommendations more efficiently.\n",
    "\n",
    "##### Approach1:\n",
    "To do this the complete similarity matrix is passed to the function. Then I sorted the matrix by the index respective to the name of U (Equal to a ranking). From this ranking the number of desired neighbourhood members is extracted from the dataframe. Ultimately, the colums of the leftover rankings are called and filtered for the top N.\n",
    "\n",
    "##### Approach2:\n",
    "The second approach uses a given threshold in order to select the neighbourhood. This threshold was selected as > .5 and < .-5 correlation in this case. (default on False due to bad test results)\n",
    "\n",
    "##### The following efficiency steps have been taken during the experimentation:\n",
    "Initial function validating topMatches with for loop comparison:\n",
    "4.11 ms ± 153 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
    "\n",
    "Final function, validating top matches based on sorting the dataframe:\n",
    "356 µs ± 6.14 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topMatches(prefs, similarity_matrix, person,n=10,neighbourhood_size=20, threshold_option=False, threshold=.5):\n",
    "    \"\"\"\n",
    "    Computes neighbourhood of U that is inserted into getRecommendations & calls get Recommendations.\n",
    "    \"\"\"\n",
    "    #if function for optional threshold\n",
    "    if threshold_option == True:\n",
    "        #check similarity matrixa against threshold and selct index as neighbourhood\n",
    "        neighbourhood = similarity_matrix[abs(similarity_matrix.loc[person, :]) > threshold].index\n",
    "        \n",
    "        #define final dataframe with defined columns\n",
    "        final_ratings = prefs.loc[:, neighbourhood]  \n",
    "    \n",
    "    elif threshold_option == False:\n",
    "        #sorting values in the similarity matrix by row (axis 1) of respective person (ascending for ranking from high to low)\n",
    "        similarity_matrix.sort_values(axis=1, by=person, inplace=True, ascending=False)\n",
    " \n",
    "        #call columns of ranked neigbours(since U is also included)\n",
    "        neighbourhood = similarity_matrix.columns[:neighbourhood_size+1]\n",
    "        \n",
    "        #define the final dataframe as all the columns until neighbourhood_size \n",
    "        final_ratings = prefs.loc[:, neighbourhood]\n",
    " \n",
    "    return final_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it together and calling the functions\n",
    "The following cells defines a function that calls all the functions in the right order.\n",
    "The function needs the dataset as well as the \"mode\" as input.\n",
    "\n",
    "The function also measures the runtime in seconds. Final result for the sample dataset hereby 0.023 seconds and for the 100k dataset 3.208 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for Mock Dataset\n",
      "Recommendations for User 4489: Valery:\n",
      "[(3.4536672646823403, '780: Independence Day (ID4) (1996)'), (2.9366979049586686, '2916: Total Recall (1990)'), (1.6663555021333516, '34: Babe (1995)'), (0.0, '3578: Gladiator (2000)')]\n",
      "Total Runtime: 0.023396629999979268 seconds\n",
      "Running for 100k Dataset\n",
      "Recommendations for User 53:\n",
      "[(0.8684884347818576, '313: Titanic (1997)'), (0.7361540994819582, '300: Air Force One (1997)'), (0.7151724515586922, '286: English Patient, The (1996)'), (0.6932283960434489, '302: L.A. Confidential (1997)'), (0.6413848603413959, '272: Good Will Hunting (1997)'), (0.600547031366321, '750: Amistad (1997)'), (0.5789836886972937, '315: Apt Pupil (1998)'), (0.5654497503736058, '316: As Good As It Gets (1997)'), (0.5502212752592008, '304: Fly Away Home (1996)'), (0.5410753444132799, '269: Full Monty, The (1997)')]\n",
      "Total Runtime: 3.121182277999992 seconds\n"
     ]
    }
   ],
   "source": [
    "def run_recommender(prefs, mode):\n",
    "    #### Run with Mock Dataset ####\n",
    "    start = time.perf_counter()\n",
    "    print(f'Running for {mode} Dataset')\n",
    "    \n",
    "    #randomply choose a user from the selected dataset to estimate ratings for\n",
    "    person_U = random.choice(prefs.columns)\n",
    "    \n",
    "    #compute complete correlation matrix\n",
    "    corrMatrix = pearsonMatrix(prefs)\n",
    "\n",
    "    #computing the neighbourhood of U\n",
    "    final_neigh_matrix = topMatches(prefs, person=person_U, similarity_matrix=corrMatrix)\n",
    "\n",
    "    #call getRecommendations function with neighbourhood dataframe\n",
    "    recommendations = getRecommendations(prefs=final_neigh_matrix, person=person_U, similarity_matrix=corrMatrix)\n",
    "\n",
    "    #print final results\n",
    "    print(f'Recommendations for User {person_U}:')\n",
    "    print(recommendations)\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    print(f'Total Runtime: {end-start} seconds')\n",
    "    return\n",
    "\n",
    "run_recommender(critics, mode='Mock')\n",
    "run_recommender(critics_100k, mode='100k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: Critics Regression Comparison (Scatter Plot)\n",
    "The following cells firstly plot the movie ratings of two randomly chosen users from the database on a scatterplot. Secondly, the resulting scatterplot is validated computing the pearsonSimilarity between the users. As expected the result of the plot explains the pearsonCorrelation.\n",
    "\n",
    "The Scatterplot of two User's ratings represents the similarity between the two respective users.\n",
    "As more the regression of the pair is on one line in the plot as closer will be result of pearsonSimilarity be to 1 or - 1 (depending on the nature of the line). On the other side, as more clustered the regression is in the plot as closer will the regression be to 0.\n",
    "\n",
    "This makes sense since the correlation represents nothing else than the rsquared regression coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "corrMatrix = pearsonMatrix(critics_100k) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc43456df10>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "user1 = random.choice(critics_100k.columns) # choosing a random user 1 from the dataset\n",
    "user2 = random.choice(critics_100k.columns) # choosing a random user 1 from the dataset\n",
    "plt.plot(critics_100k[user1], critics_100k[user2], '.') #plotting the scatter of the rating of user1 & user2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1923182294268097"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonSimilarity(user1, user2, corrMatrix) #compute the pearson correlation of user1 & user2 movie ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
